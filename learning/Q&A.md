1. 写操作为什么是单一队长写，而不是多个队长同时写，是因为当时硬件的限制导致的吗?
“当时硬件的限制”只是很小一部分原因，更核心的原因是为了保证“全局顺序性（Global Ordering）”和“顺序 IO 的极致优化”，以及代码实现的“简单性（Simplicity）”。

我们把 LevelDB 采用“单一队长（Single Leader）写”的原因拆解来看，主要有以下三点：

1. 维护“全局顺序” (Sequence Number) 的硬性需求
LevelDB 是一个 LSM-Tree 结构的数据库，它支持 MVCC（多版本并发控制）。这意味着：

每一次写入（Put/Delete），系统都会分配一个全局唯一的、单调递增的 序列号（Sequence Number）。

数据在 WAL（日志）中的顺序、在 MemTable 中的顺序，必须严格与这个序列号的顺序一致。

如果允许多个队长同时写： 假设线程 A 抢到了序号 100，线程 B 抢到了序号 101。

如果线程 B 动作快，先把 101 写入了日志，而线程 A 还在磨蹭。

这时候系统突然崩了。

恢复时： 数据库看到了 101，却没看到 100。这就造成了数据空洞（Hole）或者状态不一致。

单一队长的优势： 只有队长一人分配序号，分配完立刻写 WAL，写完立刻插 MemTable。这让“序号分配”和“数据落盘”变成了一个原子化的串行操作，从根本上消除了数据乱序的风险，极大地降低了 crash recovery（崩溃恢复）的逻辑复杂度。

2. 磁盘 IO 的物理特性（顺序写 vs 随机写）
不管是在 2011 年（LevelDB 发布时）还是现在，磁盘（尤其是机械硬盘 HDD，但也包括 SSD）的一个铁律是：顺序写（Sequential Write）远快于随机写。

WAL (Write Ahead Log) 是一个不断追加的文件。

如果是多线程并发写同一个文件，必须要在文件系统层面或者应用层面加锁，否则内容会写花（Interleaved）。

即使并发写，最终落到磁盘磁头上，也是串行的。

Group Commit 的智慧： 既然磁盘最终只能“排队写”，不如在内存里先排好队。

多线程竞争写文件 = 频繁的锁竞争 + 频繁的上下文切换（Context Switch）。

单一队长批量写 = 一次锁获取 + 一次大块 IO 写入。

这不仅不是因为硬件差，反而是为了迁就硬件特性，为了榨干磁盘的顺序写带宽。

3. MemTable (SkipList) 的实现复杂度
LevelDB 的内存表使用的是 SkipList（跳表）。 请注意 skiplist.h 中的注释，LevelDB 的跳表设计原则是：

多读并发： 支持无锁（Lock-free）的多线程并发读取。

单写： 不支持多线程并发插入。写操作必须由外部加锁同步。

如果要支持并发写 MemTable： 就需要使用 CAS (Compare-And-Swap) 等原子操作来设计极其复杂的 Lock-free SkipList，或者使用更细粒度的锁（如分段锁）。这会带来几个问题：

代码极难写对： 并发数据结构是 bug 的温床。

CPU 缓存抖动： 多核同时修改同一个数据结构，会导致 CPU 缓存行（Cache Line）频繁失效（False Sharing），在某些场景下性能反而不如单线程顺序插入。

总结与延伸：RocksDB 的改进
LevelDB 的作者（Jeff Dean 等）崇尚 "Simple is beautiful"。单一队长模型在逻辑上是最清晰的，且在当时的硬件环境下（单盘吞吐量通常是瓶颈），单线程写内存的速度通常远大于磁盘写的速度，所以写内存并不是瓶颈。

但是（划重点）： 随着 SSD（NVMe）越来越快，磁盘已经不是瓶颈了，CPU 和 锁竞争 成了瓶颈。

这也是为什么 Facebook 基于 LevelDB 开发 RocksDB 时，针对这一点做了巨大的修改：

RocksDB 引入了 allow_concurrent_memtable_write 选项。

它允许在 MemTable 层面进行并发写入（通过更复杂的 SkipList 实现）。

它引入了 Pipeline Write 机制，把 WAL 的写入和 MemTable 的写入解耦，试图进一步利用多核 CPU。

所以，您的直觉有一半是对的：在多核 CPU 数十个、NVMe SSD 极其快速的今天，LevelDB 的“单一队长”确实在极致性能上不如 RocksDB 这种针对现代硬件优化的引擎，但在当时，它是工程实现与硬件特性的完美平衡。
