# 写操作为什么是单一队长写，而不是多个队长同时写，是因为当时硬件的限制导致的吗?
“当时硬件的限制”只是很小一部分原因，更核心的原因是为了保证“全局顺序性（Global Ordering）”和“顺序 IO 的极致优化”，以及代码实现的“简单性（Simplicity）”。\
我们把 LevelDB 采用“单一队长（Single Leader）写”的原因拆解来看，主要有以下三点：
- 维护“全局顺序” (Sequence Number) 的硬性需求\
LevelDB 是一个 LSM-Tree 结构的数据库，它支持 MVCC（多版本并发控制）。这意味着：每一次写入（Put/Delete），系统都会分配一个全局唯一的、单调递增的 序列号（Sequence Number）。数据在 WAL（日志）中的顺序、在 MemTable 中的顺序，必须严格与这个序列号的顺序一致。如果允许多个队长同时写： 假设线程 A 抢到了序号 100，线程 B 抢到了序号 101。如果线程 B 动作快，先把 101 写入了日志，而线程 A 还在磨蹭。这时候系统突然崩了。恢复时： 数据库看到了 101，却没看到 100。这就造成了数据空洞（Hole）或者状态不一致。\
单一队长的优势： 只有队长一人分配序号，分配完立刻写 WAL，写完立刻插 MemTable。这让“序号分配”和“数据落盘”变成了一个原子化的串行操作，从根本上消除了数据乱序的风险，极大地降低了 crash recovery（崩溃恢复）的逻辑复杂度。
- 磁盘 IO 的物理特性（顺序写 vs 随机写）\
不管是在 2011 年（LevelDB 发布时）还是现在，磁盘（尤其是机械硬盘 HDD，但也包括 SSD）的一个铁律是：顺序写（Sequential Write）远快于随机写。WAL (Write Ahead Log) 是一个不断追加的文件。如果是多线程并发写同一个文件，必须要在文件系统层面或者应用层面加锁，否则内容会写花（Interleaved）。即使并发写，最终落到磁盘磁头上，也是串行的。Group Commit 的智慧： 既然磁盘最终只能“排队写”，不如在内存里先排好队。\
多线程竞争写文件 = 频繁的锁竞争 + 频繁的上下文切换（Context Switch）。单一队长批量写 = 一次锁获取 + 一次大块 IO 写入。这不仅不是因为硬件差，反而是为了迁就硬件特性，为了榨干磁盘的顺序写带宽。
- MemTable (SkipList) 的实现复杂度\
LevelDB 的内存表使用的是 SkipList（跳表）。 请注意 skiplist.h 中的注释，LevelDB 的跳表设计原则是：多读并发： 支持无锁（Lock-free）的多线程并发读取。
单写： 不支持多线程并发插入。写操作必须由外部加锁同步。如果要支持并发写 MemTable： 就需要使用 CAS (Compare-And-Swap) 等原子操作来设计极其复杂的 Lock-free SkipList，或者使用更细粒度的锁（如分段锁）。这会带来几个问题：
   1. 代码极难写对： 并发数据结构是 bug 的温床。
   2. CPU 缓存抖动： 多核同时修改同一个数据结构，会导致 CPU 缓存行（Cache Line）频繁失效（False Sharing），在某些场景下性能反而不如单线程顺序插入。
## 总结与延伸：RocksDB 的改进
LevelDB 的作者（Jeff Dean 等）崇尚 "Simple is beautiful"。单一队长模型在逻辑上是最清晰的，且在当时的硬件环境下（单盘吞吐量通常是瓶颈），单线程写内存的速度通常远大于磁盘写的速度，所以写内存并不是瓶颈。但是（划重点）： 随着 SSD（NVMe）越来越快，磁盘已经不是瓶颈了，CPU 和 锁竞争 成了瓶颈。这也是为什么 Facebook 基于 LevelDB 开发 RocksDB 时，针对这一点做了巨大的修改：RocksDB 引入了 allow_concurrent_memtable_write 选项。它允许在 MemTable 层面进行并发写入（通过更复杂的 SkipList 实现）。它引入了 Pipeline Write 机制，把 WAL 的写入和 MemTable 的写入解耦，试图进一步利用多核 CPU。\
所以，您的直觉有一半是对的：在多核 CPU 数十个、NVMe SSD 极其快速的今天，LevelDB 的“单一队长”确实在极致性能上不如 RocksDB 这种针对现代硬件优化的引擎，但在当时，它是工程实现与硬件特性的完美平衡。

# 既然 Batch 里只存了起始号，那每个 KV 的序号怎么算？
- 当 LevelDB 将这个 Batch 里的数据插入到内存表（MemTable）时，会进行**“运行时展开”**。在 WriteBatch::InsertInto 函数（或者 MemTableInserter）中，逻辑大致是这样的：
```
uint64_t seq = BatchHeader.sequence; // 拿到基准序号 101
for (record in batch) {
    // 第 1 个操作 (Put A)
    MemTable.Add(seq, kTypeVal, "A", "val1"); // A 的序号是 101
    seq++; // 序号自增
    // 第 2 个操作 (Delete B)
    MemTable.Add(seq, kTypeDeletion, "B", ""); // B 的序号是 102
    seq++;
    // 第 3 个操作 (Put C)
    MemTable.Add(seq, kTypeVal, "C", "val2"); // C 的序号是 103
    seq++;
}
```
## 为什么要这样做？
这就是 LevelDB 作为一个高性能存储引擎精打细算的地方：
- 省空间（压缩率高）： 一个 WriteBatch 可能包含成千上万个 KV。如果在物理上给每个 KV 都存一个 8 字节（64位）的 SequenceNumber，空间浪费太大了。 通过只存一个 基准序号 (8 bytes)，剩下的通过顺序隐含，极大地减小了 WAL 日志的大小。
- 逻辑清晰： last_sequence += Count 这一行代码明确地表示：这一个 Batch 一次性“预订”了一段连续的序列号区间。

# leveldb为什么采用跳表而不是B＋树，虽然作者有采用简单算法的思路，但这一条理由并没有说服我；此外，为什么MySQL不采用跳表呢？
这是一个非常硬核且极具深度的问题！这触及了数据库内核设计的根本权衡（Trade-off）。\
仅仅用“实现简单”确实不足以完全解释为什么 LevelDB 选择了跳表（SkipList），毕竟 B+ 树虽然复杂，但作为工业界标准，现成的实现方案多如牛毛。\
核心原因其实在于 并发控制的粒度（Concurrency Granularity） 和 数据存储介质（Memory vs Disk）的物理特性。\
我们需要把这个问题拆分为两个战场来看：内存战场（LevelDB MemTable） vs 磁盘战场（MySQL InnoDB）。\
## 第一战场：LevelDB 为什么要用跳表？结论 1： 在内存且高并发写入的场景下，跳表以“概率平衡”换取了“结构调整的低成本”，从而获得了更好的并发性能。
背景： LevelDB 的跳表仅用于 MemTable（内存表）。它的生命周期很短，只存在于内存中，写满（4MB）就落盘销毁。
- 并发写性能：概率平衡 vs. 严格平衡\
这是最本质的技术差异，也是说服你的关键理由。
  1. B+ 树（严格平衡）：B+ 树为了维持树的高度平衡，插入数据时可能触发节点分裂（Split）或合并（Merge）。牵一发而动全身： 想象一下，你往 B+ 树叶子节点插一个数据，叶子满了要分裂，父节点也满了要继续分裂，甚至可能一直分裂到根节点。锁竞争（Latch Contention）： 为了保证并发安全，当发生结构性变化时，通常需要锁住整条路径（甚至全局锁/写锁）。虽然有像 B-Link Tree 这样的优化，但实现极其复杂，且在极高并发写入下，锁竞争依然是瓶颈。
  2. 跳表（概率平衡）：跳表的平衡是基于概率的（随机决定层高）。局部性极强： 插入一个节点，只需要修改前后相邻节点的指针。它永远不需要像 B+ 树那样进行全局性的树旋转或大规模节点分裂。CAS 友好： 因为改动非常局部，非常容易利用 CAS（Compare-And-Swap） 指令实现 无锁（Lock-free） 或 极细粒度锁 的并发控制。LevelDB 的跳表虽然是单线程写（由外部各种限制导致），但在 RocksDB（LevelDB 的继承者）中，正是利用跳表的这个特性，实现了真正的多线程并发写入 MemTable，性能吊打 B+ 树。
- 内存碎片与空间利用率\
  1. B+ 树： 基于 Page（页）管理。为了减少分裂频率，通常会预留空闲空间（比如页填充率只到 70%）。这在珍贵的内存中（MemTable 只有几 MB）是一种浪费。
  2. 跳表： 节点大小按需分配（LevelDB 使用 Arena 顺序分配）。紧凑，没有额外的页头部开销，也没有预留空洞。
- 写放大与 CPU Cache\
在纯内存场景下，B+ 树并没有绝对优势。虽然 B+ 树对 CPU Cache 更友好（节点内是数组），但跳表的实现更轻量。LevelDB 的跳表节点是连续分配在 Arena 里的，其实缓存局部性并没有想象中那么差。
## 第二战场：为什么 MySQL（InnoDB）坚持用 B+ 树？在磁盘场景下，最小化随机 IO 次数是第一铁律。B+ 树的“高扇出、低高度”特性完美契合磁盘物理特性，而跳表在磁盘上就是一场性能灾难。
背景： MySQL 的数据和索引主要存储在 磁盘（Disk/SSD） 上。
- 磁盘 IO 的物理特性（最关键理由）磁盘（尤其是机械硬盘，SSD 也有类似特性）最怕的是 随机 IO（Random IO），最喜欢的是 顺序 IO 和 块读取（Block Read）。
  1. 跳表（指针的噩梦）：跳表本质上是链表。节点 A 指向节点 B，节点 B 可能在磁盘的任何位置。查找过程： 要查找一个数据，可能要跨越很多个节点。在内存里，跳几次指针只需几十纳秒；但在磁盘上，跳一次指针可能就是一次 随机 IO。灾难： 如果用跳表存 1TB 数据，查一个 Key 可能需要几十次随机磁盘读取，耗时可能达到几百毫秒，这是不可接受的。
  2. B+ 树（扇出的奇迹）：矮胖结构： B+ 树的一个节点（Page）通常大小是 16KB，可以存上千个指针（扇出 Fan-out 很大）。高度极低： 存 2000 万行数据，B+ 树的高度可能只有 3 层。这意味着最多只需要 3 次磁盘 IO 就能找到数据。块读取友好： 操作系统读取磁盘最小单位是页（4KB），B+ 树的一个节点正好对应一个或几个物理页，一次 IO 就能把这就这一个节点里的上百个 Key 读进内存。
- 范围查询（Range Scan）
  1. 跳表： 只能通过最底层的单链表遍历，跨度大时依然面临随机 IO 问题。
  2. B+ 树： 叶子节点紧密排列，且通过双向链表连接。做范围查询时（SELECT * FROM t WHERE id > 100），基本就是磁盘上的顺序读取，速度极快。
## 总结对比

|----|----|----|
|特性|LevelDB MemTable (内存)|MySQL InnoDB Index (磁盘)|
|首要目标|极速写入、高并发支持|极少磁盘 IO、读多写少|
|数据结构|跳表 (SkipList)|B+ 树|
|为什么不用另一个？|B+ 树太重： 维护树平衡导致锁竞争激烈，代码复杂，内存碎片多。|跳表太高： 指针分散导致大量随机 IO，无法利用磁盘预读和页缓存。|
|并发策略|概率平衡，修改局部化，易于实现无锁/细粒度锁。|严格平衡，修改波及大，通常需要复杂的锁机制（Latch coupling）。|
|缓存亲和性|一般（指针跳转多，Cache miss 较高）。|极好（节点内二分查找，利用 CPU Cache Line）。|

专家视角的补充（Nuance）其实 LevelDB 的作者（Jeff Dean 等）并非没有考虑过缓存问题。\
LevelDB 的妥协： 虽然 MemTable 选了跳表，但磁盘上的 SSTable 实际上是一种有序的数组块，并带有索引。这其实有点像 B+ 树思想的变体（只有叶子节点，索引稀疏）。RocksDB 的进化： RocksDB（基于 LevelDB）为了进一步压榨性能，引入了 InlineSkipList 和各种优化，甚至在某些特定场景下（如只读），提供了基于数组的 MemTable 选项，就是为了解决跳表指针跳转带来的 CPU Cache Miss 问题。

# 什么是 Varint32？
Varint（Variable-length Integer）是一种用 1 到 5 个字节来表示一个 32 位整数的压缩编码方式。\
规则 1（有效位）： 每个字节只有 低 7 位 用于存数据。\
规则 2（标志位）： 每个字节的 最高位（第 8 位，bit 7） 是标志位（Continuation Bit）。如果是 1 (0x80 或 128)：表示“后面还有数据，还没读完”。如果是 0：表示“这是最后一个字节，读完结束”。\
规则 3（小端序）： 先读到的是整数的 低位 数据，后读到的是 高位 数据。
- 例子： 整数 300 (二进制 100101100)
拆分成 7 位一组：0000010 (高7位) 和 0101100 (低7位)。\
低7位先存，加上标志位 1（表示后面还有）：1 0101100 = 0xAC (172)。\
高7位后存，加上标志位 0（表示结束）：0 0000010 = 0x02 (2)。\
内存里存的就是：AC 02。

# LevelDB 的作者（Jeff Dean 等）并非没有考虑过缓存问题"这句话怎么理解？
这句话的意思是：虽然理论上跳表（SkipList）的“指针跳转”会导致 CPU 缓存命中率（Cache Hit Rate）不如数组或 B+ 树，但在 LevelDB 的工程实现中，作者通过特殊的内存分配手段（Arena），极大地缓解甚至抵消了这个缺点。\
Jeff Dean 和 Sanjay Ghemawat 是系统设计的大师，他们非常清楚 Latency Numbers Every Programmer Should Know（每个程序员都该知道的延迟数字，这也是 Jeff Dean 的名言）。他们绝不会忽略 L1/L2 Cache 的重要性。\
这里的核心在于：“逻辑上的链表”不代表“物理上的分散”。我们可以从以下三个维度来深度理解这句话：
1. 秘密武器：Arena 带来的“伪”局部性\
通常我们认为跳表缓存差，是因为节点是 new 出来的，散落在堆内存（Heap）的各个角落。访问 Node->Next 可能就是从内存地址 0x1000 跳到 0x9999，导致 CPU 预取失败。但是，LevelDB 使用了 Arena 内存池。内存布局： Arena 是一块连续的大内存（例如 4KB 的块）。分配方式： 当 LevelDB 连续插入数据时（比如批量写入），这些 SkipList 的节点是在 Arena 上顺序切分出来的。结果：虽然逻辑上它们是通过指针连接的链表。但在物理内存中，节点 A、节点 B、节点 C 很可能就是紧挨着的邻居！\
CPU 缓存效应： 当 CPU 读取节点 A 时，会将 A 附近的内存（正好是节点 B 和 C）一起加载到 Cache Line 中。\
结论： 通过 Arena，LevelDB 强行让跳表具备了类似数组的空间局部性（Spatial Locality）。作者不仅考虑了缓存问题，还用一种极其廉价（指针碰撞分配）的方式解决了它。
2. 性能瓶颈的转移：写锁 vs Cache Miss\
在数据库的 Write 路径上，真正的性能杀手通常不是 CPU Cache Miss，而是 锁竞争（Lock Contention） 和 代码复杂度带来的开销。B+ 树的代价： 为了维持极致的 Cache 友好度（数组结构），B+ 树在插入时需要移动数组元素、分裂节点、甚至递归调整树高。这需要复杂的逻辑和更多的 CPU 指令。\
跳表的收益： 插入操作极快，不需要移动数据，只需要改几个指针。\
权衡（Trade-off）： 作者认为，在 MemTable（只有几 MB 大小）这种场景下，“代码执行指令数的减少” 和 “内存分配的极速（Arena）” 所带来的收益，远大于 “稍微多几次 Cache Miss” 带来的损失。
3. 数据生命周期的视角：SSTable 才是缓存优化的终局\
MemTable 只是暂存区。数据最终的归宿是磁盘上的 SSTable。MemTable (跳表)： 牺牲一点读取的 Cache 性能，换取极致的写入性能和实现简单性。SSTable (磁盘文件)： 完全放弃链表结构，采用了严格有序的 Key-Value 数组和 Block 结构。为什么？因为 SSTable 是只读的，不需要插入，所以不需要跳表那种灵活的插入能力。这时候，作者就回到了极致的 Cache 友好路线上来了：Block 是连续的，可以被高效地读入 Page Cache，支持二分查找。
## 总结
Jeff Dean 等人的考虑是全局性的：
- 用 Arena 弥补跳表的短板（让物理内存尽可能连续）。
- 用跳表保证写入吞吐量（减少 CPU 指令，简化锁逻辑）。
- 在 SSTable 阶段找回 Cache 优势（因为那是读操作的主战场）。
所以，他们选跳表不是因为“偷懒”或“没想到缓存问题”，而是因为在 MemTable 这个特定的生命周期阶段，配合 Arena 使用，跳表是综合收益最高的选择。


# 内存池为什么要设定 kBlockSize / 4 这个界限？
LevelDB 默认的 kBlockSize 是 4096 (4KB)。也就是说，如果申请的内存超过 1024 (1KB)，Arena 就不会把这个对象放进当前的 4KB 内存块里，而是直接向操作系统单独申请一块内存给它。
1. 核心目的：防止“大对象挤占”导致的内存浪费\
Arena 的分配机制是：如果当前 Block 剩余空间不够，就扔掉当前 Block（无论剩多少都浪费了），开一个新的 4KB Block。如果没有这个 1/4 限制： 假设你当前 Block 还剩 200 字节。 你突然要申请 3KB 的内存。Arena 发现 200 < 3000，不够。Arena 废弃掉这 200 字节（浪费）。Arena 申请一个新的 4KB Block。放入 3KB 数据，新 Block 还剩 1KB。alloc_ptr_ 指向这剩余的 1KB。有了这个 1/4 限制： 同样场景，当前 Block 还剩 200 字节，你要申请 3KB。LevelDB 判定 3000 > 1024，属于“大对象”。单独申请一块 3000 字节的内存给你。关键点： alloc_ptr_ 依然指向原来的那个 Block（还剩 200 字节那个）。如果你接下来要申请一个 50 字节的小对象，你依然可以用原来那个 Block 里的空间！结果： 那 200 字节没有被浪费。
2. 为什么是 1/4？不是 1/2 或 1/8？
这是一个经验值（Heuristic），是对碎片率和系统调用开销的权衡。如果设得太大 (比如 > 3/4)： 只有极大的对象才单独分配。那么像 2KB 这种中等对象就会频繁触发“废弃旧 Block，开新 Block”的动作，导致旧 Block 尾部的大量剩余空间（比如剩 1KB）被浪费。如果设得太小 (比如 > 1/16)： 稍微大一点的对象（比如 300 字节）都要单独找操作系统 new。 这会导致 Arena 退化成普通的 malloc，失去了内存池“减少系统调用、内存连续友好”的优势，增加了 CPU 开销和内存元数据的浪费。结论： 1/4 (25%) 是一个经典的工程折中点，既保证了绝大多数小对象能享受内存池的高速，又防止了偶尔的大对象导致内存池频繁切换产生巨大碎片。

# 新分配的内存和原来的内存是不是就不存在局部连续性了？
当 Arena 申请一个新的内存块（New Block）时，这个新块和原来的旧块在物理内存地址上大概率是不连续的。这就导致了 LevelDB 的内存空间局部性不是“全局连续”的，而是**“分段连续” (Piecewise Contiguous)** 的。让我们深入分析一下这种现象，以及为什么 LevelDB 依然选择这样做。
1. 内存布局图解：从“长条”变成“群岛”。如果不使用 Arena，每个节点都 malloc，内存像散沙一样：使用 Arena 后，内存变成了**“一个个紧凑的岛屿”**：Block 1 (4KB): [Node A][Node B]...[Node M] —— 内部极度连续。Gap (断层): 这里可能跨越了巨大的内存地址空间。Block 2 (4KB): [Node N][Node O]...[Node Z] —— 内部极度连续。
当跳表在遍历时：从 A 走到 B：Cache Hit (极快)。从 M 走到 N（跨块）：Cache Miss (稍慢，CPU 需要重新加载 Cache Line)。
2. 针对您提到的 AllocateFallback 的两种情况\
这里有一个非常精妙的设计细节，针对“大对象”和“写满切换”，LevelDB 的处理方式不同，对连续性的影响也不同。\
情况 A：因为对象太大 (> 1KB) 而单独分配 (Big Object)发生了什么？ 这是一个“大对象特供通道”。LevelDB 找 OS 申请了一块“飞地”（独立的内存块）专门放这个大对象。关键点： arena_ 里的 alloc_ptr_ 依然停留在原来的旧块里。连续性分析：大对象本身： 确实是“孤悬海外”，和前后的数据都不连续。后续的小对象： 比如你插完这个 3KB 的大 Key 后，又插了一个 50 字节的小 Key。这个小 Key 会继续写在原来的旧块里，紧挨着之前的旧数据。设计意图： 牺牲大对象的局部性，为了保护小对象流的连续性。 不让偶尔出现的大胖子打断小个子们的排队。\
情况 B：因为旧块满了而分配 (Block Full)发生了什么？ 旧的“岛屿”满了，我们开辟一个新的“岛屿”。连续性分析： 这里确实出现了真正的断层。第 N 个 Key 在旧块尾部，第 N+1 个 Key 在新块头部。它们之间失去了连续性。
3. 为什么这种“分段连续”是可以接受的？您可能会问：“既然断层会导致 Cache Miss，那为什么不一开始就申请一个巨大的（比如 100MB）内存块，保证永远连续呢？”这里有几个工程上的权衡：操作系统限制： 申请极大的连续物理内存（或虚拟内存）容易失败，或者导致操作系统的内存碎片管理压力增大。扩容成本： 如果一开始申请 100MB，结果用户只写了 1KB，浪费巨大。如果支持动态扩容（类似 std::vector 的 realloc），需要把旧数据拷贝到新地址，这个拷贝成本太高了。局部性足够好了：一个 Block 是 4KB。假设一个 KV 节点平均 100 字节，一个 Block 能装 40 个节点。这意味着：每处理 40 个节点，才会遇到 1 次“断层”带来的 Cache Miss。97.5% 的访问 依然享受了完美的内存局部性。这在工程上已经是非常优秀的优化了。
